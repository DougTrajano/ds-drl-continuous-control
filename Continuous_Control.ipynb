{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "In this notebook, I'll train an agent to solve the Reacher Environment using Unity ML-Agents.\n",
    "\n",
    "---\n",
    "\n",
    "# Index\n",
    "\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()\n",
    "- []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the Environment\n",
    "\n",
    "<img align=\"left\" width=\"150\" src=\"https://www.nclouds.com/img/services/toolkit/sagemaker.png\"/>\n",
    "\n",
    "This notebook was developed on AWS SageMaker.\n",
    "\n",
    "The kernel used is **conda_python3**\n",
    "\n",
    "To setup this environment on SageMaker you need to run the next 3 cells.\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch===1.5.1 torchvision===0.6.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from agent import Agent\n",
    "\n",
    "# One agent\n",
    "#env_path = \"one_agent/Reacher_Windows_x86_64/Reacher.exe\"\n",
    "#env_path = \"one_agent/Reacher_Linux/Reacher.x86_64\"\n",
    "\n",
    "# Multi agent\n",
    "#env_path = \"multi_agent/Reacher_Windows_x86_64/Reacher.exe\"\n",
    "env_path = \"multi_agent/Reacher_Linux/Reacher.x86_64\"\n",
    "\n",
    "env = UnityEnvironment(file_name=env_path, no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define helper functions to training session\n",
    "\n",
    "Here we'll create a function that can be very helpful to teach the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_trainer(agent, n_episodes=100, max_time_step=None, print_range=10, early_stop=13, agent_reset=False, verbose=False):\n",
    "    \"\"\"Deep Q-Learning trainer.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent (object): The agent object\n",
    "        n_episodes (int): maximum number of training episodes.\n",
    "        max_time_step (int or None): The max time step per episode, if None, the agent will training until environment is done.\n",
    "        print_range (int): range to print partials results.\n",
    "        early_stop (int): Stop training when achieve a defined score respecting 10 min n_episodes.\n",
    "        agent_reset (bool): If we need to reset agent at every episode.\n",
    "        verbose (bool): If verbose true, we'll print some infos on console.\n",
    "    \"\"\"\n",
    "    scores_episode = [] # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_range) # last print_range scores\n",
    "    scores_mean = []\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    num_agents = len(env_info.agents)\n",
    "    if verbose and num_agents > 1:\n",
    "        print(\"We have more than one agent available on this environment. We'll replicate the agent to the max number of agents.\")\n",
    "    agents = []\n",
    "    for i in range(num_agents):\n",
    "        agents.append(agent)\n",
    "        \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "        time_step = 0\n",
    "        training = True # control max_time_step\n",
    "        \n",
    "        if agent_reset:\n",
    "            for agent in agents:\n",
    "                agent.reset()\n",
    "            \n",
    "        while training:\n",
    "            actions = np.array([agents[i].act(states[i]) for i in range(num_agents)])\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            for i in range(num_agents):\n",
    "                agents[i].step(states[i], actions[i], rewards[i], next_states[i], dones[i]) \n",
    "                \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            time_step += 1\n",
    "            \n",
    "            if isinstance(max_time_step, (int, float)):\n",
    "                if time_step >= max_time_step:\n",
    "                    training = False\n",
    "            if np.any(dones):\n",
    "                break\n",
    "                \n",
    "        score = np.mean(scores)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores_episode.append(score)              # save most recent score\n",
    "        if verbose:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_window)), end=\"\")\n",
    "            if ep % print_range == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window) >= early_stop and i > 10:\n",
    "            if verbose:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(ep, np.mean(scores_window)))\n",
    "            break\n",
    "            \n",
    "    return scores_episode, ep, np.mean(scores_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000):\n",
    "    \"\"\" Deep Deterministic Policy Gradients\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = np.zeros(num_agents)\n",
    "    scores_episode = []\n",
    "    \n",
    "    agents =[] \n",
    "    \n",
    "    for i in range(num_agents):\n",
    "        agents.append(Agent(state_size, action_size, seed=0))\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        for agent in agents:\n",
    "            agent.reset()\n",
    "            \n",
    "        scores = np.zeros(num_agents)\n",
    "            \n",
    "        for t in range(max_t):\n",
    "            #actions = [agents[i].act(states[i]) for i in range(num_agents)]\n",
    "            actions = np.array([agents[i].act(states[i]) for i in range(num_agents)])\n",
    "#             if t == 0:\n",
    "#                 print(\"actions\", actions)\n",
    "            env_info = env.step(actions)[brain_name]        # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done        \n",
    "            \n",
    "            for i in range(num_agents):\n",
    "                agents[i].step(states[i], actions[i], rewards[i], next_states[i], dones[i]) \n",
    " \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if t % 20:\n",
    "                print('\\rTimestep {}\\tScore: {:.2f}\\tmin: {:.2f}\\tmax: {:.2f}'\n",
    "                      .format(t, np.mean(scores), np.min(scores), np.max(scores)), end=\"\") \n",
    "            if np.any(dones):\n",
    "                break \n",
    "        score = np.mean(scores)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores_episode.append(score)\n",
    "\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_window)), end=\"\\n\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(Agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(Agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "            \n",
    "    return scores_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Agent\n",
    "\n",
    "In the next code cells, we will train the Agent to work on environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=399, lr_actor=1e-4, lr_critic=1e-4,\n",
    "              memory_size=int(1e6), batch_size=32, gamma=0.99, tau=1e-3, weight_decay=0.0,\n",
    "              actor_units=(64, 32), critic_units=(64, 32), action_min=-1, action_max=1, update_every=8)\n",
    "\n",
    "scores, episodes, last_avg_score = rl_trainer(agent, n_episodes=1000, max_time_step=None,\n",
    "                                              early_stop=30, agent_reset=True, verbose=True)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Agent losses visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Agent on Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "eps = 0.05\n",
    "while True:\n",
    "    action = agent.act(state, eps)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
