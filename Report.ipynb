{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm\n",
    "\n",
    "The algorithm that I choosed to use on this environment is DDPG (Deep Deterministic Policy Gradient), you can see more about this algorithm [here](http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring18/ujjawal/DDPG-Algorithm.pdf)\n",
    "\n",
    "See below the architecture of this algorithm\n",
    "\n",
    "![](images/DDPG_architecture.png)\n",
    "\n",
    "DDPG is an **Advantage Actor Critic Method** that combines Policy-based and Value-based in the same agent. See below a simplest explanation about how it's works.\n",
    "\n",
    "![](images/actor-critic_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters were chosen based on training experiments seeking a score greater than 5 in the least episodes possible.\n",
    "\n",
    "At the end, our hyperparameters are:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"action_max\": 1,\n",
    "    \"action_min\": -1,\n",
    "    \"action_size\": 4,\n",
    "    \"actor_units\": [\n",
    "        256,\n",
    "        128\n",
    "    ],\n",
    "    \"batch_size\": 16,\n",
    "    \"critic_units\": [\n",
    "        256,\n",
    "        128\n",
    "    ],\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr_actor\": 0.0001,\n",
    "    \"lr_critic\": 0.0001,\n",
    "    \"memory_size\": 1000000,\n",
    "    \"random_seed\": 399,\n",
    "    \"state_size\": 33,\n",
    "    \"tau\": 0.001,\n",
    "    \"weight_decay\": 0.0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks architectures\n",
    "\n",
    "We have two Neural Networks for this agent, Actor and Critic. See below the architecture of each of these neural networks.\n",
    "\n",
    "### Actor Neural Network\n",
    "\n",
    "```\n",
    "Actor(\n",
    "    (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
    "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
    "    (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "### Critic Neural Network\n",
    "\n",
    "```\n",
    "Critic(\n",
    "  (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Rewards\n",
    "\n",
    "The environment choosed is the **Version 2: Twenty (20) Agents** of the Reacher Environment.\n",
    "\n",
    "See below the score graph and how much episodes was needed to solve the environment.\n",
    "\n",
    "![](images/DDPG_score_multiagent.png)\n",
    "\n",
    "The complete training history is available on [Continuous_Control.ipynb](Continuous_Control.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "The submission has concrete future ideas for improving the agent's performance.\n",
    "\n",
    "I have some ideas for future work to improve the agent's performance.\n",
    "\n",
    "- New combinations of hyperparameters.\n",
    "- Implement [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
